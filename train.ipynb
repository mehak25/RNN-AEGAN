{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from nemoursModel.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import import_ipynb\n",
    "from nemoursModel import *\n",
    "import random\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from pickle import dump,load\n",
    "#from helper_functions import *\n",
    "#from dataset import get_data_loader\n",
    "import torchvision.utils as utils\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "from argparse import ArgumentParser\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "save_path = \"data/saved_models/nemours20.tar\"\n",
    "\n",
    "if not os.path.exists(\"data/saved_models\"):\n",
    "    os.makedirs(\"data/saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--feature_matching'], dest='feature_matching', nargs=0, const=True, default=False, type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARG_PARSER = ArgumentParser()\n",
    "\n",
    "ARG_PARSER.add_argument('--nfeatures', default=947, type=int)\n",
    "ARG_PARSER.add_argument('--dfeatures', default=43, type=int)\n",
    "ARG_PARSER.add_argument('--ehidden', default=74, type=int)\n",
    "\n",
    "\n",
    "ARG_PARSER.add_argument('--num_epochs', default=100, type=int)\n",
    "ARG_PARSER.add_argument('--seq_len', default=40, type=int)\n",
    "ARG_PARSER.add_argument('--pred_len', default=30, type=int)\n",
    "ARG_PARSER.add_argument('--batch_size', default=16, type=int)\n",
    "ARG_PARSER.add_argument('--patience', default=20, type=int)\n",
    "ARG_PARSER.add_argument('--e_lrn_rate', default=0.1, type=float)\n",
    "ARG_PARSER.add_argument('--g_lrn_rate', default=0.1, type=float)\n",
    "ARG_PARSER.add_argument('--d_lrn_rate', default=0.001, type=float)\n",
    "ARG_PARSER.add_argument('--resume_training', default=False)\n",
    "ARG_PARSER.add_argument('--train', default=False)\n",
    "ARG_PARSER.add_argument('--eval', default=True)\n",
    "ARG_PARSER.add_argument('--autoencoder', default=False)\n",
    "ARG_PARSER.add_argument('--noise', default=False)\n",
    "ARG_PARSER.add_argument('--clip_value', default=0.01, type=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS = ARG_PARSER.parse_args(args=[])\n",
    "MAX_SEQ_LEN = ARGS.seq_len\n",
    "BATCH_SIZE = ARGS.batch_size\n",
    "EPSILON = 1e-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(args, model):\n",
    "    model['e'].eval()\n",
    "    model['g'].eval()\n",
    "    model['d'].eval()\n",
    "    \n",
    "    RLoss=0\n",
    "    TBatches=0\n",
    "    oBmi=[]\n",
    "    iBmi=[]\n",
    "    oAge=[]\n",
    "    oSex=[]\n",
    "    \n",
    "    output=[]\n",
    "    with T.autograd.no_grad():\n",
    "        files = 'test/testGan20.csv'\n",
    "    \n",
    "        maskFiles = 'test/testGanMask20.csv'\n",
    "        \n",
    "        dataset = CSVDataset(files, int(args.seq_len*500),1356100,args.seq_len)\n",
    "        maskDataset = CSVDataset(maskFiles, int(args.seq_len*500),1356100, args.seq_len)\n",
    "\n",
    "        loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "        maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "        loss={}\n",
    "\n",
    "        #for every batch\n",
    "        for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "            data,mask=allData\n",
    "            data=data.squeeze()\n",
    "            mask=mask.squeeze()\n",
    "            \n",
    "            #values to be predicted\n",
    "            y = data.clone().detach()\n",
    "            sex=y[:,:,947]\n",
    "            age=y[:,:,945]\n",
    "            y=y[:,:,653]\n",
    "            data=data[:,:,0:947]\n",
    "            \n",
    "            #yOrig=yOrig[:,:,653]\n",
    "\n",
    "\n",
    "\n",
    "            #------------remove data from to be predicted timestamps------------------\n",
    "            for i in range(data.shape[0]):\n",
    "                j=40\n",
    "                k=30\n",
    "                \n",
    "                data[i,j-k:j,:]=0\n",
    "                y[i,0:j-k]=0\n",
    "                age[i,0:j-k]=0\n",
    "                sex[i,0:j-k]=0\n",
    "                mask[i,0:j-k,:]=0\n",
    "\n",
    "\n",
    "\n",
    "            #------------E training------------------\n",
    "            h,(h_n,c_n) = model['e'](data)\n",
    "\n",
    "            #------------G training------------------\n",
    "            z=h_n[1].repeat(1,args.seq_len).view(h_n[1].shape[0],args.seq_len,-1)\n",
    "            h_n=T.unsqueeze(h_n[1],0)\n",
    "            c_n=T.unsqueeze(c_n[1],0)   \n",
    "            output.extend(h_n)\n",
    "            decodedOutput = model['g'](z,(h_n,c_n))\n",
    "            \n",
    "            #------------R Loss------------------\n",
    "            criterion = nn.MSELoss()\n",
    "            y=y.cpu()\n",
    "            age=age.cpu()\n",
    "            sex=sex.cpu()\n",
    "            decodedOutput=decodedOutput.cpu()\n",
    "            mask=mask.cpu()\n",
    "\n",
    "            #------------MASk values-----------------\n",
    "            y=y.reshape(-1)\n",
    "            age=age.reshape(-1)\n",
    "            sex=sex.reshape(-1)\n",
    "            mask=mask[:,:,653]\n",
    "            mask=mask.reshape(-1)\n",
    "            y=np.multiply(y,mask)\n",
    "            age=np.multiply(age,mask)\n",
    "            sex=np.multiply(sex,mask)\n",
    "\n",
    "\n",
    "            outputBMI=decodedOutput\n",
    "            outputBMI=outputBMI.reshape(-1)\n",
    "            outputBMI=np.multiply(outputBMI,mask)\n",
    "            \n",
    "\n",
    "            #------------Supervised Loss------------------\n",
    "            \n",
    "            loss = (outputBMI-y)\n",
    "            loss =  (loss).pow(2)\n",
    "            loss = loss.sum()\n",
    "            loss = (loss)/(np.count_nonzero(mask))\n",
    "            loss = T.sqrt(loss)\n",
    "            RLoss=RLoss+loss\n",
    "            outBmi, inBmi, outAge, outSex = plotBmi(outputBMI, y, age, sex)\n",
    "            oBmi.extend(outBmi)\n",
    "            iBmi.extend(inBmi)\n",
    "            oAge.extend(outAge)\n",
    "            oSex.extend(outSex)\n",
    "\n",
    "        TBatches=TBatches+batch_idx+1\n",
    "        \n",
    "    RLoss = RLoss/TBatches\n",
    "    #print(\"===================================\")\n",
    "    print(\"Val R Loss:\",RLoss)\n",
    "    return oBmi, iBmi, oAge, oSex\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBmi(outBmi , inBmi, outAge, outSex):\n",
    "    \n",
    "    outBmi = outBmi.cpu().detach().numpy()\n",
    "    inBmi = inBmi.cpu().detach().numpy()\n",
    "    outAge = outAge.cpu().detach().numpy()\n",
    "    outSex = outSex.cpu().detach().numpy()\n",
    "    \n",
    "    outBmi=outBmi[outBmi!=0]\n",
    "    inBmi=inBmi[inBmi!=0]\n",
    "    outAge=outAge[outAge!=0]\n",
    "    outSex=outSex[outSex!=0]\n",
    "    \n",
    "    print(outBmi)\n",
    "    print(inBmi)\n",
    "    print(outAge)\n",
    "    print(outSex)\n",
    "    \n",
    "    return outBmi,inBmi, outAge, outSex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evalFull(args, model):\n",
    "    model['e'].eval()\n",
    "    model['g'].eval()\n",
    "    model['d'].eval()\n",
    "    \n",
    "    RLoss=0\n",
    "    TBatches=0\n",
    "    oBmi=[]\n",
    "    iBmi=[]\n",
    "    oAge=[]\n",
    "    oSex=[]\n",
    "    \n",
    "    output=[]\n",
    "    with T.autograd.no_grad():\n",
    "        files = 'val/testGan20.csv'\n",
    "    \n",
    "        maskFiles = 'val/testGanMask20.csv'\n",
    "        \n",
    "        dataset = CSVDataset(files, int(args.seq_len*500),1356100,args.seq_len)\n",
    "        maskDataset = CSVDataset(maskFiles, int(args.seq_len*500),1356100, args.seq_len)\n",
    "\n",
    "        loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "        maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "        loss={}\n",
    "\n",
    "        #for every batch\n",
    "        for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "            #print('batch: {}'.format(batch_idx))\n",
    "            data,mask=allData\n",
    "            data=data.squeeze()\n",
    "            mask=mask.squeeze()\n",
    "            \n",
    "            #values to be predicted\n",
    "            y = data.clone().detach()\n",
    "            #Take only BMI Values\n",
    "            y=y[:,:,653]\n",
    "            #data=data[:,:,0:947]\n",
    "            \n",
    "            #------------remove data from to be predicted timestamps------------------\n",
    "            for i in range(data.shape[0]):\n",
    "                j=40\n",
    "                k=30\n",
    "                \n",
    "                data[i,j-k:j,:]=0\n",
    "                y[i,0:j-k]=0\n",
    "                age[i,0:j-k]=0\n",
    "                mask[i,0:j-k,:]=0\n",
    "        \n",
    "            #------------E training------------------\n",
    "            h,(h_n,c_n) = model['e'](data)\n",
    "\n",
    "            #------------G training------------------\n",
    "            z=h_n[1].repeat(1,args.seq_len).view(h_n[1].shape[0],args.seq_len,-1)\n",
    "            h_n=T.unsqueeze(h_n[1],0)\n",
    "            c_n=T.unsqueeze(c_n[1],0)   \n",
    "            output.extend(h_n)\n",
    "            decodedOutput = model['g'](z,(h_n,c_n))\n",
    "\n",
    "            #------------R Loss------------------\n",
    "            criterion = nn.MSELoss()\n",
    "            y=y.cpu()\n",
    "            age=age.cpu()\n",
    "            sex=sex.cpu()\n",
    "            decodedOutput=decodedOutput.cpu()\n",
    "            mask=mask.cpu()\n",
    "\n",
    "            #------------MASK Values------------------\n",
    "            y=y.reshape(-1)\n",
    "            #Mask only for BMI values\n",
    "            mask=mask[:,:,653]\n",
    "            mask=mask.reshape(-1)\n",
    "            y=np.multiply(y,mask)\n",
    "            outputBMI=decodedOutput\n",
    "            outputBMI=outputBMI.reshape(-1)\n",
    "            outputBMI=np.multiply(outputBMI,mask)\n",
    "\n",
    "            #------------Supervised Loss------------------\n",
    "            loss = (outputBMI-y)\n",
    "            loss =  (loss).pow(2)\n",
    "            loss = loss.sum()\n",
    "            loss = (loss)/(np.count_nonzero(mask))\n",
    "            loss = T.sqrt(loss)\n",
    "            RLoss=RLoss+loss\n",
    "        \n",
    "\n",
    "        TBatches=TBatches+batch_idx+1\n",
    "    RLoss = RLoss/TBatches\n",
    "    #print(\"===================================\")\n",
    "    print(\"Val R Loss:\",RLoss)\n",
    "    return RLoss\n",
    "                \n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def batch_run(model,data,mask,args, optimizer, criterion,RLoss, DLoss, GLoss):\n",
    "    \n",
    "    loss={}\n",
    "    freeze_d=False\n",
    "    data=T.mul(data,mask)\n",
    "\n",
    "    #------------E training------------------\n",
    "    h,(h_n,c_n) = model['e'](data)\n",
    "\n",
    "    #Take only BMI\n",
    "    data=data[:,:,653]\n",
    "    \n",
    "    data=T.unsqueeze(data,2)\n",
    "    #print(\"Input Data: \",data.shape)\n",
    "    \n",
    "    #------------G training Inputs------------------\n",
    "    z=h_n[1].repeat(1,args.seq_len).view(h_n[1].shape[0],args.seq_len,-1)\n",
    "    h_n=T.unsqueeze(h_n[1],0)\n",
    "    c_n=T.unsqueeze(c_n[1],0) \n",
    "\n",
    "    #------------ G training------------------\n",
    "    decodedOutput = model['g'](z,(h_n,c_n))\n",
    "    #paramsG=list(model['g'].parameters())\n",
    "\n",
    "    #------------Move to CUDA------------------\n",
    "    data=data.cuda()\n",
    "    mask=mask[:,:,653]\n",
    "    mask=T.unsqueeze(mask,2)\n",
    "    mask=mask.cpu()\n",
    "\n",
    "    #------------MASK Genrated output------------------\n",
    "    mask=mask.reshape(-1)\n",
    "    y = data.clone().detach()\n",
    "    y=y.cpu()\n",
    "    y=y.reshape(-1)\n",
    "    y=np.multiply(y,mask)\n",
    "    \n",
    "    output=decodedOutput.clone().detach()\n",
    "    output=output.cpu()\n",
    "    output=output.reshape(-1)\n",
    "    decoded=output * mask\n",
    "\n",
    "\n",
    "    #------------Prepare labels for Discriminator------------------\n",
    "    y_real = get_cuda(T.ones(data.shape[0]))\n",
    "    y_fake = get_cuda(T.zeros(data.shape[0]))\n",
    "\n",
    "\n",
    "    #------------R Loss------------------\n",
    "    y=y.cuda()\n",
    "    decoded=decoded.cuda()\n",
    "    loss['g']=criterion['g'](y,decoded)\n",
    "    RLoss=RLoss+math.sqrt(loss['g'].item())\n",
    "\n",
    "    #------------D training------------------\n",
    "    if not args.autoencoder:\n",
    "        #print(\"Auto\")\n",
    "        if args.noise:\n",
    "            #print(\"Noise\")\n",
    "            #------------Random Noise G training------------------\n",
    "            h_noise = T.empty([data.shape[0], 400]).uniform_() # random vector\n",
    "            z_noise=h_noise.repeat(1,args.seq_len).view(h_noise.shape[0],args.seq_len,-1)\n",
    "            h_noise=T.unsqueeze(h_noise,0)\n",
    "            c_noise = T.empty([1, c_n.shape[1], c_n.shape[2]]).uniform_()\n",
    "            noiseOutput = model['g'](z_noise,(h_noise,c_noise))\n",
    "            nLabels = model['d'](noiseOutput.detach(),flag=\"fake\")\n",
    "\n",
    "            rLabels = model['d'](data,flag=\"original\")\n",
    "            fLabels = model['d'](decodedOutput,flag=\"fake\")\n",
    "            loss['d'] = criterion['d'](rLabels,fLabels,nLabels)\n",
    "            \n",
    "            noise_loss= -T.mean(nLabels)\n",
    "        \n",
    "        #if No Noise\n",
    "        optimizer['d'].zero_grad()\n",
    "        rLabels = model['d'](data,flag=\"original\")\n",
    "        fLabels = model['d'](decodedOutput,flag=\"fake\")\n",
    "        loss['d'] = criterion['d'](rLabels,fLabels)\n",
    "        \n",
    "        DLoss=DLoss+loss['d'].item()\n",
    "\n",
    "        loss['d'].backward(retain_graph=True)\n",
    "        optimizer['d'].step()\n",
    "        \n",
    "\n",
    "        #------------GD Loss------------------\n",
    "        #nLabels = model['d'](noiseOutput,flag=\"fake\")\n",
    "        decodedOutput = model['g'](z,(h_n,c_n))\n",
    "        fLabels = model['d'](decodedOutput,flag=\"fake\")\n",
    "        gen_loss= -T.mean(fLabels)\n",
    "        #noise_loss= -T.mean(nLabels)\n",
    "\n",
    "        loss['gd'] = gen_loss# + noise_loss\n",
    "        GLoss=GLoss+loss['gd'].item()\n",
    "        loss['g']=loss['g']+loss['gd']\n",
    "\n",
    "    #------------E G training------------------\n",
    "    optimizer['e'].zero_grad()\n",
    "    optimizer['g'].zero_grad()\n",
    "    loss['g'].backward()\n",
    "    optimizer['e'].step()\n",
    "    optimizer['g'].step()\n",
    "    \n",
    "    return RLoss, DLoss, GLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def run_epoch(args, model, optimizer, criterion):\n",
    "    ''' Run a single epoch\n",
    "    '''\n",
    "    \n",
    "    decodedOutput=[]\n",
    "    trainLoss=[]\n",
    "    valLoss=[]\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=args.patience, verbose=True)\n",
    "    if args.resume_training:\n",
    "        early_stopping(133, model, optimizer, save_path)\n",
    "    #for evrey epoch\n",
    "    for epoch in range(args.num_epochs):\n",
    "        model['e'].train()\n",
    "        model['g'].train()\n",
    "        model['d'].train()\n",
    "    \n",
    "        #Running Losses\n",
    "        RLoss=0\n",
    "        DLoss=0\n",
    "        GLoss=0\n",
    "        TBatches=0  \n",
    "        print(\"=============EPOCH=================\")\n",
    "        freeze_d=False\n",
    "        #if epoch<=args.g_pretraining_epochs:\n",
    "            #freeze_d=True\n",
    "       \n",
    "        path = 'train/Data20/'\n",
    "        files = list(map(lambda x : path + x, (filter(lambda x : x.endswith(\"csv\"), os.listdir(path)))))\n",
    "        \n",
    "        maskPath = 'train/Data20/mask/'\n",
    "        maskFiles = list(map(lambda x : maskPath + x, (filter(lambda x : x.endswith(\"csv\"), os.listdir(maskPath)))))\n",
    "        \n",
    "        ids=[0,1,2,3]\n",
    "        for i in ids:\n",
    "            \n",
    "            #print(\"====================New File========================\")\n",
    "            dataset = CSVDataset(files[i], int(args.seq_len*500),1356100,args.seq_len)\n",
    "            maskDataset = CSVDataset(maskFiles[i], int(args.seq_len*500),1356100, args.seq_len)\n",
    "\n",
    "            loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "            maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "            #for every batch\n",
    "            for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "                #print('batch: {}'.format(batch_idx))\n",
    "                data,mask=allData\n",
    "\n",
    "                data=data.squeeze()\n",
    "                mask=mask.squeeze()\n",
    "\n",
    "                RLoss, DLoss, GLoss = batch_run(model,data,mask,args, optimizer, criterion,RLoss, DLoss, GLoss)\n",
    "                \n",
    "                T.cuda.empty_cache()\n",
    "\n",
    "            TBatches=TBatches+batch_idx+1\n",
    "    \n",
    "        RLoss=RLoss/TBatches\n",
    "        DLoss=DLoss/TBatches\n",
    "        GLoss=GLoss/TBatches   \n",
    "        \n",
    "        trainLoss.append(RLoss)\n",
    "\n",
    "        valid_loss = run_evalFull(args, model)\n",
    "        \n",
    "        valLoss.append(valid_loss)\n",
    "\n",
    "        print(\"epoch:\", epoch, \"loss_R:\", \"%.4f\"%RLoss, \"loss_G:\", \"%.4f\"%GLoss, \"loss_D:\", \"%.4f\"%DLoss)\n",
    "\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return trainLoss, valLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisLoss(nn.Module):\n",
    "    ''' C-RNN-GAN discriminator loss\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(DisLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits_real, logits_gen):#, logits_noise):\n",
    "        ''' Discriminator loss\n",
    "        logits_real: logits from D, when input is real\n",
    "        logits_gen: logits from D, when input is from Generator\n",
    "        '''\n",
    "        d_loss_real = logits_real\n",
    "\n",
    "        d_loss_gen = logits_gen# + logits_noise\n",
    "\n",
    "        batch_loss = -(d_loss_real - d_loss_gen)\n",
    "        return torch.mean(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    ''' Training sequence\n",
    "    '''\n",
    "    train_on_gpu = T.cuda.is_available()\n",
    "    if train_on_gpu:\n",
    "        print('Training on GPU.')\n",
    "    else:\n",
    "        print('No GPU available, training on CPU.')\n",
    "        \n",
    "    #Create Models\n",
    "    model = {\n",
    "        'e': Encoder(args.nfeatures, args.ehidden, use_cuda=train_on_gpu),\n",
    "        'g': Generator(args.nfeatures, args.ehidden, use_cuda=train_on_gpu),\n",
    "        'd': Discriminator(args.nfeatures, args.ehidden, use_cuda=train_on_gpu)\n",
    "    }\n",
    "\n",
    "    \n",
    "    optimizer = {\n",
    "        'e': optim.RMSprop(model['e'].parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0.01, momentum=0.9, centered=False),\n",
    "        'g': optim.RMSprop(model['g'].parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0.01, momentum=0.9, centered=False),\n",
    "        'd': optim.AdamW(model['d'].parameters(), args.d_lrn_rate)\n",
    "        \n",
    "    }\n",
    "    criterion = {\n",
    "        'g': nn.MSELoss(reduction='sum'),\n",
    "        'd': DisLoss()\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model['e'].cuda()\n",
    "        model['g'].cuda()\n",
    "        model['d'].cuda()\n",
    "        \n",
    "    if args.resume_training:\n",
    "        checkpoint = T.load(save_path)\n",
    "        model['e'].load_state_dict(checkpoint['E_model'])\n",
    "        model['g'].load_state_dict(checkpoint['G_model'])\n",
    "        model['d'].load_state_dict(checkpoint['D_model'])\n",
    "        optimizer['e'].load_state_dict(checkpoint['E_trainer'])\n",
    "        optimizer['g'].load_state_dict(checkpoint['G_trainer'])\n",
    "        optimizer['d'].load_state_dict(checkpoint['D_trainer'])\n",
    "        #Save Updated Model\n",
    "        \n",
    "        output = run_epoch(args, model, optimizer, criterion) \n",
    "        \n",
    "        return model,output\n",
    "    \n",
    "    elif args.train:\n",
    "        trainloss, valLoss = run_epoch(args, model, optimizer, criterion) \n",
    "        \n",
    "        return trainloss, valLoss\n",
    "        \n",
    "    elif args.eval:\n",
    "        #load Model\n",
    "        checkpoint = T.load(save_path)\n",
    "        model['e'].load_state_dict(checkpoint['E_model'])\n",
    "        model['g'].load_state_dict(checkpoint['G_model'])\n",
    "        model['d'].load_state_dict(checkpoint['D_model'])\n",
    "        optimizer['e'].load_state_dict(checkpoint['E_trainer'])\n",
    "        optimizer['g'].load_state_dict(checkpoint['G_trainer'])\n",
    "        optimizer['d'].load_state_dict(checkpoint['D_trainer'])\n",
    "        oBmi, iBmi, oAge, oSex = run_test(args, model)\n",
    "        \n",
    "        return oBmi, iBmi, oAge, oSex\n",
    "    \n",
    "    #'epoch': epoch + 1,\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oBmi, iBmi, oAge, oSex = main(ARGS)\n",
    "trainloss, valLoss = main(ARGS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
